minbucket = round(minsplit/3),
cp = 0.01,
maxcompete = 4,
maxsurrogate = 5,
usesurrogate = 2,
xval = 10,
surrogatestyle = 0,
maxdepth = 30)
Control.ID3 = rpart.control(minsplit = 20,
# minbucket = round(minsplit/3),
cp = 0.01,
maxcompete = 4,
maxsurrogate = 5,
usesurrogate = 2,
xval = 10,
surrogatestyle = 0,
maxdepth = 30)
Tree.ID3 <- rpart(formula = formula,
data = training,
method="class",
na.action = na.rpart,
parms=list(split="information"),
control = Control.ID3
)
Tree.ID3
printcp(wine_decisiontree_ID3)
printcp(Tree.ID3)
Tree.ID3 <- rpart(formula = formula,
data = training,
method="class",
na.action = na.rpart,
parms=list(split="information")
# control = Control.ID3
)
Tree.ID3
printcp(Tree.ID3)
printcp(Tree.ID3)
printcp(Tree.ID3)
Control.ID3 = rpart.control(minsplit = 20, # min obs. in node
minbucket = 6, # min obs. in leaf, defaut is round(minsplit/3)
cp = 0.01, # Lower cp first. Later prune tree by larger one.
maxcompete = 4, # ???
maxsurrogate = 5, # ???
usesurrogate = 2, # ???
xval = 10, # cross validation
surrogatestyle = 0, # ???
maxdepth = 30)
Tree.ID3 <- rpart(formula = formula,
data = training,
method="class",
na.action = na.rpart,
parms=list(split="information")
# control = Control.ID3
)
Tree.ID3
Tree.ID3 <- rpart(formula = formula,
data = training,
method="class",
na.action = na.rpart,
parms=list(split="information")
control = Control.ID3
)
Tree.ID3
Control.ID3 = rpart.control(minsplit = 20, # min obs. in node
minbucket = 6, # min obs. in leaf, defaut is round(minsplit/3)
cp = 0.01, # Lower cp first. Later prune tree by larger one.
maxcompete = 4, # ???
maxsurrogate = 5, # ???
usesurrogate = 2, # ???
xval = 10, # cross validation
surrogatestyle = 0, # ???
maxdepth = 30 #The root node counted as depth 0
)
Tree.ID3 <- rpart(formula = formula,
data = training,
method="class",
na.action = na.rpart,
parms=list(split="information")
control = Control.ID3
)
Tree.ID3 <- rpart(formula = formula,
data = training,
method="class",
na.action = na.rpart,
parms=list(split="information"),
control = Control.ID3
)
Tree.ID3
rpart.plot(Tree.ID3,
branch=1,
fallen.leaves=T,
cex=0.6)
library(rpart.plot)
rpart.plot(Tree.ID3,
branch=1,
fallen.leaves=T,
cex=0.6)
install.packages('rpart.plot')
rpart.plot(Tree.ID3,
branch=1,
fallen.leaves=T,
cex=0.6)
library(rpart.plot)
rpart.plot(Tree.ID3,
branch=1,
fallen.leaves=T,
cex=0.6)
Tree.ID3 <- rpart(formula = formula,
data = training,
method="class",
na.action = na.rpart,
parms=list(split="information"),
#control = Control.ID3
)
Tree.ID3
rpart.plot(Tree.ID3,
branch=1,
fallen.leaves=T,
cex=0.6)
Tree.ID3 <- rpart(formula = formula,
data = training,
method="class",
na.action = na.rpart,
parms=list(split="information"),
control = Control.ID3
)
Control.ID3 = rpart.control(minsplit = 20, # min obs. in node
minbucket = 6, # min obs. in leaf, defaut is round(minsplit/3)
cp = 0.01, # Lower cp first. Later prune tree by larger one.
maxcompete = 4, # ???
maxsurrogate = 5, # ???
usesurrogate = 2, # ???
xval = 10, # cross validation
surrogatestyle = 0, # ???
maxdepth = 30 #The root node counted as depth 0
)
Tree.ID3 <- rpart(formula = formula,
data = training,
method="class",
na.action = na.rpart,
parms=list(split="information"),
control = Control.ID3
)
Tree.ID3
rpart.plot(Tree.ID3,
branch=1,
fallen.leaves=T,
cex=0.6)
Control.ID3 = rpart.control(minsplit = 20, # min obs. in node
minbucket = 6, # min obs. in leaf, defaut is round(minsplit/3)
cp = 0.001, # Lower cp first. Later prune tree by larger one.
maxcompete = 4, # ???
maxsurrogate = 5, # ???
usesurrogate = 2, # ???
xval = 10, # cross validation
surrogatestyle = 0, # ???
maxdepth = 30 #The root node counted as depth 0
)
Tree.ID3 <- rpart(formula = formula,
data = training,
method="class",
na.action = na.rpart,
parms=list(split="information"),
control = Control.ID3
)
Tree.ID3
rpart.plot(Tree.ID3,
branch=1,
fallen.leaves=T,
cex=0.6)
Control.ID3 = rpart.control(minsplit = 20, # min obs. in node
minbucket = 6, # min obs. in leaf, defaut is round(minsplit/3)
cp = 0.005, # Lower cp first. Later prune tree by larger one.
maxcompete = 4, # ???
maxsurrogate = 5, # ???
usesurrogate = 2, # ???
xval = 10, # cross validation
surrogatestyle = 0, # ???
maxdepth = 30 #The root node counted as depth 0
)
Tree.ID3 <- rpart(formula = formula,
data = training,
method="class",
na.action = na.rpart,
parms=list(split="information"),
control = Control.ID3
)
Tree.ID3
library(rpart.plot)
rpart.plot(Tree.ID3,
branch=1,
fallen.leaves=T,
cex=0.6)
Control.ID3 = rpart.control(minsplit = 20, # min obs. in node
minbucket = 6, # min obs. in leaf, defaut is round(minsplit/3)
cp = 0.001, # Lower cp first. Later prune tree by larger one.
maxcompete = 4, # ???
maxsurrogate = 5, # ???
usesurrogate = 2, # ???
xval = 10, # cross validation
surrogatestyle = 0, # ???
maxdepth = 30 #The root node counted as depth 0
)
Tree.ID3 <- rpart(formula = formula,
data = training,
method="class",
na.action = na.rpart,
parms=list(split="information"),
control = Control.ID3
)
Tree.ID3
library(rpart.plot)
rpart.plot(Tree.ID3,
branch=1,
fallen.leaves=T,
cex=0.6)
printcp(Tree.ID3, digits = 0.01)
printcp(Tree.ID3)
Control.ID3 = rpart.control(minsplit = 20, # min obs. in node
minbucket = 6, # min obs. in leaf, defaut is round(minsplit/3)
cp = 0.001, # Lower cp first. Later prune tree by larger one.
maxcompete = 4, # ???
maxsurrogate = 5, # ???
usesurrogate = 2, # ???
xval = 10, # cross validation
surrogatestyle = 0, # ???
maxdepth = 30 #The root node counted as depth 0
)
Tree.ID3 <- rpart(formula = formula,
data = training,
method="class",
na.action = na.rpart,
parms=list(split="information"),
control = Control.ID3
)
Tree.ID3
library(rpart.plot)
rpart.plot(Tree.ID3,
branch=1,
fallen.leaves=T,
cex=0.6)
printcp(Tree.ID3)
rpart.plot(Tree.ID3,
branch=1,
fallen.leaves=T,
cex=0.6)
printcp(Tree.ID3)
pred.ID3 <- predict(Tree.ID3,
newdata=testing,
type="class")
pred.ID3
ModelPerformance <- function(predict_condition, true_condition){
require(dplyr)
require(tidyr)
require(ggplot2)
# Confusion Matrix, Contingency Table
confusion_matrix <- table(predict_condition, true_condition)
# Hit, True Positive
TP <-  confusion_matrix[1,1]
# Correct Rejection, True Negative
TN <- confusion_matrix[2,2]
# False Alarm, Type I Error, False Positive
FP <-  confusion_matrix[1,2]
# Miss, Type II Error, False Negative
FN <-  confusion_matrix[2,1]
# Sensitivity, Recall, True Positive Rate
TPR <- TP / (TP + FN)
# Specificity, Selectivity, True Negative Rate
TNR <- TN / (TN + FP)
# Precision, Positive Predictive Value
PPV <- TP / (TP + FP)
# Negative Prediction Value
NPV <- TN / (TN + FN)
# Miss Rate, False Negative Rate
FNR <- FN /(FN + TP)
# Fall Out, False Positive Rate
FPR <-  FP / (FP + TN)
# False Discovery Rate
FDR <- FP / (FP + TP)
# False Omission Rate
FOR <- FN / (FN + TN)
# Threat Score, Critical Success Index
TS <- TP / (TP + FN + FP)
# Accuracy
ACC <- (TP + TN) / sum(confusion_matrix)
# Balanced Accuracy, Harmonic Mean of Precision and Sensitivity
BA <- (TPR + TNR) / 2
# F1 Score
F1 <- 2*(PPV*TPR)/(PPV+TPR)
# Matthews correlation coefficient
MCC <- (TP*TN -FP*FN) / (sqrt(TP+FP)*sqrt(TP+FN)*sqrt(TN+FP)*sqrt(TN+FN))
# Bookmaker Informedness
BM <- TPR + TNR - 1
# Markedness, delta-P
MK <- PPV + NPV - 1
df = data.frame(
TPR = TPR,
TNR = TNR,
PPV = PPV,
NPV = NPV,
FNR = FNR,
FPR = FPR,
FDR = FDR,
FOR = FOR,
TS = TS,
ACC = ACC,
BA = BA,
F1 = F1,
MCC = MCC,
BM = BM,
MK = MK
)
positive_measure = df[c('TPR', 'TNR', 'PPV', 'NPV', 'F1',
'ACC', 'TS', 'BA', "BM", 'MK')]%>% gather
plot_positive <-  ggplot(positive_measure,
aes(x=reorder(key,-value),y=value,fill=key))+
geom_bar(stat="identity")+
# coord_polar(theta="x",direction=1)+
labs(x="Measure",y="Performance")+
theme(legend.position="bottom",legend.box="horizontal")+
ggtitle(label = 'Positive Model Performance',
subtitle = 'The higher, the better.')+
geom_text(aes(x = reorder(key,-value),
y=value,
label = round(value, 2)),
vjust = 1.2)
negative_measure = df[c("FNR", "FPR", "FDR", "FOR")]%>% gather
plot_negative <- ggplot(negative_measure,
aes(x=reorder(key,-value),y=value,fill=key))+
geom_bar(stat="identity")+
# coord_polar(theta="x",direction=1)+
labs(x="Measure",y="Performance")+
theme(legend.position="bottom",legend.box="horizontal")+
ggtitle(label = 'Negative Model Performance',
subtitle = 'The lower, the better.')+
geom_text(aes(x = reorder(key,-value),
y=value,
label = round(value, 2)),
vjust = 1.2)
return(
list(
confusion_matrix = confusion_matrix,
TP = TP,
TN = TN,
FP = FP,
FN = FN,
TPR = TPR,
TNR = TNR,
PPV = PPV,
NPV = NPV,
FNR = FNR,
FPR = FPR,
FDR = FDR,
FOR = FOR,
TS = TS,
ACC = ACC,
BA = BA,
F1 = F1,
MCC = MCC,
BM = BM,
MK = MK,
df = df,
plot_positive = plot_positive,
plot_negative = plot_negative
)
)
}
ModelPerformance(pred.ID3, testing$irradiate)
perform.ID3 <- ModelPerformance(pred.ID3, testing$irradiate)
perform.ID3$confusion_matrix
# Training
perform.ID3.Apparent <- ModelPerformance(pred.ID3, training$irradiate)
# Training
pred.ID3 <- predict(Tree.ID3,
newdata=training,
type="class")
perform.ID3.Apparent <- ModelPerformance(pred.ID3, training$irradiate)
perform.ID3.Apparent$confusion_matrix
perform.ID3.Apparent$plot_positive
perform.ID3.Apparent$plot_negative
# True Performance
perform.ID3.True <- ModelPerformance(pred.ID3, testing$irradiate)
perform.ID3.True$confusion_matrix
# True Performance
pred.ID3 <- predict(Tree.ID3,
newdata=testing,
type="class")
perform.ID3.True <- ModelPerformance(pred.ID3, testing$irradiate)
perform.ID3.True$confusion_matrix
perform.ID3.True$plot_positive
perform.ID3.True$plot_negative
printcp(Tree.ID3)
Tree.ID3.prune <- prune(Tree.ID3,cp= 0.01)
rpart.plot(Tree.ID3.prune,
branch=1,
fallen.leaves=T,
cex=0.6)
# Apparent Performance
pred.ID3.prune <- predict(Tree.ID3.prune,
newdata=training,
type="class")
perform.ID3.prune.Apparent <- ModelPerformance(pred.ID3, training$irradiate)
perform.ID3.prune.Apparent$confusion_matrix
perform.ID3.prune.Apparent$plot_positive
# Apparent Performance
pred.ID3.prune <- predict(Tree.ID3.prune,
newdata=training,
type="class")
# Apparent Performance
pred.ID3.prune <- predict(Tree.ID3.prune,
newdata=training,
type="class")
perform.ID3.prune.Apparent <- ModelPerformance(pred.ID3, training$irradiate)
perform.ID3.prune.Apparent <- ModelPerformance(pred.ID3.prune, training$irradiate)
perform.ID3.prune.Apparent$confusion_matrix
perform.ID3.prune.Apparent$plot_positive
perform.ID3.prune.Apparent$plot_negative
Tree.ID3_prune <- prune(Tree.ID3,cp= 0.01)
# Apparent Performance after pruning
pred.ID3_prune.train <- predict(Tree.ID3_prune,
newdata=training,
type="class")
perform.ID3_prune.Apparent <- ModelPerformance(pred.ID3_prune.train, training$irradiate)
perform.ID3_prune.Apparent$confusion_matrix
perform.ID3_prune.Apparent$plot_positive
# True Performance after pruning
pred.ID3_prune.test <- predict(Tree.ID3_prune,
newdata=testing,
type="class")
perform.ID3_prune.True <- ModelPerformance(pred.ID3_prune.test, testing$irradiate)
perform.ID3_prune.True$confusion_matrix
perform.ID3_prune.True$plot_positive
perform.ID3_prune.True$ACC
perform.ID3_prune.Apparen$ACC
# Apparent Performance after pruning
pred.ID3_prune.train <- predict(Tree.ID3_prune,
newdata=training,
type="class")
perform.ID3_prune.Apparent <- ModelPerformance(pred.ID3_prune.train, training$irradiate)
perform.ID3_prune.Apparent$confusion_matrix
perform.ID3_prune.Apparent$plot_positive
perform.ID3_prune.Apparent$plot_negative
perform.ID3_prune.Apparen$ACC
perform.ID3_prune.Apparent$ACC
perform.ID3.True$ACC
perform.ID3.Apparent$ACC
perform.ID3.Apparent$ACC
# Apparent Performance
pred.ID3.train <- predict(Tree.ID3,
newdata=training,
type="class")
perform.ID3.Apparent <- ModelPerformance(pred.ID3.train, training$irradiate)
perform.ID3.Apparent$confusion_matrix
perform.ID3.Apparent$plot_positive
perform.ID3.Apparent$plot_negative
perform.ID3.Apparent$ACC
# True Performance
pred.ID3.test <- predict(Tree.ID3.prune,
newdata=testing,
type="class")
perform.ID3.True <- ModelPerformance(pred.ID3.test, testing$irradiate)
perform.ID3.True$confusion_matrix
perform.ID3.True$plot_positive
perform.ID3.True$plot_negative
Tree.ID3_prune <- prune(Tree.ID3,cp= 0.01)
Tree.ID3_prune
Tree.ID3_prune <- prune(Tree.ID3,cp= 0.01)
Tree.ID3_prune
rpart.plot(Tree.ID3_prune,
branch=1,
fallen.leaves=T,
cex=0.6)
rpart.plot(Tree.ID3_prune,
branch=1,
fallen.leaves=T,
cex=0.6)
rpart.plot(Tree.ID3,
branch=1,
fallen.leaves=T,
cex=0.6)
rpart.plot(Tree.ID3_prune,
branch=1,
fallen.leaves=T,
cex=0.6)
plot.ID3_prune
plot.ID3_prune = rpart.plot(Tree.ID3_prune,
branch=1,
fallen.leaves=T,
cex=0.6)
plot.ID3_prune
plot.ID3 = rpart.plot(Tree.ID3,
branch=1,
fallen.leaves=F,
cex=0.6)
plot.ID3
plot.ID3 = rpart.plot(Tree.ID3,
branch=1,
fallen.leaves=F,
cex=0.6)
plot.ID3
rpart.plot(Tree.ID3,
branch=1,
fallen.leaves=F,
cex=0.6)
dev.off()
rpart.plot(Tree.ID3_prune,
branch=1,
fallen.leaves=T,
cex=0.6)
dev.off()
set.seed(20200409)
summary(BreastCancer)
