---
title: "DecisionTree"
author: "Mike"
date: "2020/4/6"
output: 
  html_document: 
    theme: united
    toc: yes
    toc_float: yes
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Reference

### Main Concept

1. [决策树（decision tree）(一)——构造决策树方法](https://blog.csdn.net/u012328159/article/details/70184415)

2. [决策树（decision tree）(二)——剪枝](https://blog.csdn.net/u012328159/article/details/79285214)

3. [决策树（decision tree）(三)——连续值处理](https://blog.csdn.net/u012328159/article/details/79396893)

4. [决策树（decision tree）（四）——缺失值处理](https://blog.csdn.net/u012328159/article/details/79413610)

5. [HOW DECISION TREE ALGORITHM WORKS](http://dataaspirant.com/2017/01/30/how-decision-tree-algorithm-works/)


### ID3

1. [分类算法 -- 决策树ID3算法](https://blog.csdn.net/weixin_43216017/article/details/87474045)

1. [ID3 Algorithm & ROC Analysis ](https://www.slideshare.net/talhakabakus/id3-algorithm-roc-analysis-14984288)

### C4.5

1. [决策树建树原理之C4.5和C5.0以及CART建树原理（二](https://blog.csdn.net/weixin_44835596/article/details/89852921)



### CART

1. [Decision Tree 決策樹 | CART, Conditional Inference Tree, RandomForest](https://www.jamleecute.com/decision-tree-cart-%E6%B1%BA%E7%AD%96%E6%A8%B9/)

1. [CART 分類與回歸樹](https://www.itread01.com/content/1509178954.html)



### CHAID 

1. [IBM SPSS Modeler算法系列--决策树CHAID算法](https://www.cda.cn/view/20421.html)

1. [卡方自動互動檢測](https://www.newton.com.tw/wiki/%E5%8D%A1%E6%96%B9%E8%87%AA%E5%8B%95%E4%BA%92%E5%8B%95%E6%AA%A2%E6%B8%AC/21125021)


1. [CHAID and R – When you need explanation – May 15, 2018](https://www.r-bloggers.com/chaid-and-r-when-you-need-explanation-may-15-2018/)


### C50

1. [IBM SPSS Modeler算法系列-----决策树C5.0算法](https://blog.csdn.net/chenjunji123456/article/details/52189312?depth_1-utm_source=distribute.pc_relevant.none-task-blog-OPENSEARCH-12&utm_source=distribute.pc_relevant.none-task-blog-OPENSEARCH-12)



### Code

1. [决策树ID3原理及R语言python代码实现（西瓜书）](https://www.bbsmax.com/A/ke5jv4wy5r/)

1. [R语言：决策树ID3/C4.5/CART/C5.0算法的实现](https://blog.csdn.net/weixin_43216017/article/details/87739323)

1. [NTU CINC : R軟體資料探勘實務(上)--分類模型](http://www.cc.ntu.edu.tw/chinese/epaper/0034/20150920_3410.html)

1. [Generate ROC Curve Charts for Print and Interactive Use](https://cran.r-project.org/web/packages/plotROC/vignettes/examples.html)

1. [決策樹(Decision Tree)以及隨機森林(Random Forest)介紹](https://medium.com/jameslearningnote/%E8%B3%87%E6%96%99%E5%88%86%E6%9E%90-%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E7%AC%AC3-5%E8%AC%9B-%E6%B1%BA%E7%AD%96%E6%A8%B9-decision-tree-%E4%BB%A5%E5%8F%8A%E9%9A%A8%E6%A9%9F%E6%A3%AE%E6%9E%97-random-forest-%E4%BB%8B%E7%B4%B9-7079b0ddfbda)

1. [卡方自動交叉檢驗 (CHAID)](https://rpubs.com/jiankaiwang/chaid)

1. [Determining Creditworthiness for Loan Applications Using C5.0 Decision Trees](https://rpubs.com/cyobero/C50)

1. [Package ‘C50’](https://cran.r-project.org/web/packages/C50/C50.pdf)

### Others

1. [機器學習：決策樹總結｜ID3 C4.5/C5.0 CHAID CART與QUEST](https://kknews.cc/zh-tw/tech/m36obez.html)


1. [Regression Tree | 迴歸樹, Bagging, Bootstrap Aggregation | R語言](https://www.jamleecute.com/regression-tree-%E8%BF%B4%E6%AD%B8%E6%A8%B9-bagging-bootstrap-aggrgation-r%E8%AA%9E%E8%A8%80/)


1.  [rpart 决策树中的 Cp（complexity parameter）参数](https://blog.csdn.net/a8131357leo/article/details/89529677)


1. [Meaning of Surrogate Split](https://stats.stackexchange.com/questions/171574/meaning-of-surrogate-split)

1. [消除誤差修剪法 Reduced error pruning (REP)](http://123android.blogspot.com/2011/11/111027-data-mining.html)

1. [C4.5 Pessimistic Error Pruning，PEP](http://gitlinux.net/2019-06-04-C45/)


---

# Algorithm

| Algorithm | Explainatory Var. Type |       Feature Selection       | Pruning Rule | Missing Value |
| --------- | ---------------------- | ----------------------------- | ------------ | ------------- |
|    ID3    |       Discrete         |    Entropy, Information Gain  | Predicted ER |  Not Allowed  |
|   C4.5    | Discrete, Bi-partition |    Entropy, IV, Gain Ratio    | **PEP, REP** |    Allowed    |
|   CHAID   | Discrete, Ch-Merge     |    Chisquare Test p value     |  No Pruning  |       ?       |
|   CART    |       Continuous       |       Gini Index, MSE         | Predicted ER |       ?       |
|   QUEST   |          ?             |  Chisquare / F Test p value   | Predicted ER |       ?       |
|   C5.0    | Discrete, Bi-partition | Gain Ratio, PRISM?, Boosting? |     PEP      |       ?       |
|  PUBLIC   |          ?             |               ?               |      ?       |       ?       |
|   SLIQ    |          ?             |               ?               |      ?       |       ?       |
|  SPRINT   |          ?             |               ?               |      ?       |       ?       |



# Packages & Functions

```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=FALSE}
library(magrittr) # %<>%
library(dplyr) # data manipulation
library(rpart) # ID3, CART
# library(RWeka) # C4.5(J48)
# library(C50) # C5.0
```

## Partition Data 

Ramdomly partition data into Training and Testing
```{r}
Partition = function(data, train_proportion = 0.9){
  train_index = sample(x = nrow(data), 
                       size = train_proportion*nrow(data),
                       replace = FALSE )
  train_data = data[train_index,]
  test_data = data[-train_index,]
  return(
    list(
      training = train_data,
      testing = test_data
      )
    )
}
  

```

## Model Performance

```{r message=FALSE, warning=FALSE}

ModelPerformance <- function(predict_condition, true_condition){
  
  require(dplyr)
  require(tidyr)
  require(ggplot2)
    
  # Confusion Matrix, Contingency Table
  confusion_matrix <- table(predict_condition, true_condition)
  
  if(identical(dim(confusion_matrix), as.integer(c(2,2)))){
    
    # 2x2 confusion matrix --------------------------------
    
    # Hit, True Positive
    TP <-  confusion_matrix[1,1]
    # Correct Rejection, True Negative
    TN <- confusion_matrix[2,2]
    # False Alarm, Type I Error, False Positive
    FP <-  confusion_matrix[1,2]
    # Miss, Type II Error, False Negative
    FN <-  confusion_matrix[2,1]
    
    
    
    # Sensitivity, Recall, True Positive Rate
    TPR <- TP / (TP + FN)
    
    # Specificity, Selectivity, True Negative Rate
    TNR <- TN / (TN + FP)
    
    # Precision, Positive Predictive Value
    PPV <- TP / (TP + FP)
    
    # Negative Prediction Value
    NPV <- TN / (TN + FN)
    
    # Miss Rate, False Negative Rate
    FNR <- FN /(FN + TP)
    
    # Fall Out, False Positive Rate
    FPR <-  FP / (FP + TN)
    
    # False Discovery Rate
    FDR <- FP / (FP + TP)
    
    # False Omission Rate
    FOR <- FN / (FN + TN)
    
    
    
    # Threat Score, Critical Success Index
    TS <- TP / (TP + FN + FP)
    
    # Accuracy
    ACC <- (TP + TN) / sum(confusion_matrix)
    
    # Balanced Accuracy, Harmonic Mean of Precision and Sensitivity
    BA <- (TPR + TNR) / 2
    
    # F1 Score
    F1 <- 2*(PPV*TPR)/(PPV+TPR)
    
    # Matthews correlation coefficient
    MCC <- (TP*TN -FP*FN) / (sqrt(TP+FP)*sqrt(TP+FN)*sqrt(TN+FP)*sqrt(TN+FN))
    
    # Bookmaker Informedness
    BM <- TPR + TNR - 1
    
    # Markedness, delta-P
    MK <- PPV + NPV - 1
    
    df = data.frame(
          TPR = TPR, 
          TNR = TNR, 
          PPV = PPV, 
          NPV = NPV, 
          FNR = FNR, 
          FPR = FPR, 
          FDR = FDR, 
          FOR = FOR, 
          TS = TS, 
          ACC = ACC, 
          BA = BA, 
          F1 = F1, 
          MCC = MCC, 
          BM = BM, 
          MK = MK
        )
  
  
    positive_measure = df[c('TPR', 'TNR', 'PPV', 'NPV', 'F1',
                            'ACC', 'TS', 'BA', "BM", 'MK')]%>% gather
  
    plot_positive <-  ggplot(positive_measure, 
         aes(x=reorder(key,-value),y=value,fill=key))+
          geom_bar(stat="identity")+
          # coord_polar(theta="x",direction=1)+
          labs(x="Measure",y="Performance")+
          theme(legend.position="bottom",legend.box="horizontal")+
          ggtitle(label = 'Positive Model Performance',
                  subtitle = 'The higher, the better.')+
          geom_text(aes(x = reorder(key,-value),
                              y=value, 
                              label = round(value, 2)),
                    vjust = 1.2)
    
    
    negative_measure = df[c("FNR", "FPR", "FDR", "FOR")]%>% gather
  
    plot_negative <- ggplot(negative_measure, 
         aes(x=reorder(key,-value),y=value,fill=key))+
          geom_bar(stat="identity")+
          # coord_polar(theta="x",direction=1)+
          labs(x="Measure",y="Performance")+
          theme(legend.position="bottom",legend.box="horizontal")+
          ggtitle(label = 'Negative Model Performance',
                  subtitle = 'The lower, the better.')+
          geom_text(aes(x = reorder(key,-value),
                              y=value, 
                              label = round(value, 2)),
                    vjust = 1.2)
  
  
    return(
      list(
        confusion_matrix = confusion_matrix,
        TP = TP, 
        TN = TN, 
        FP = FP, 
        FN = FN, 
        
        TPR = TPR, 
        TNR = TNR, 
        PPV = PPV, 
        NPV = NPV, 
        FNR = FNR, 
        FPR = FPR, 
        FDR = FDR, 
        FOR = FOR, 
        TS = TS, 
        ACC = ACC, 
        BA = BA, 
        F1 = F1, 
        MCC = MCC, 
        BM = BM, 
        MK = MK,
    
        df = df,
        plot_positive = plot_positive,
        plot_negative = plot_negative
      )
    )
  }else if(identical(dim(confusion_matrix), as.integer(c(3,3)))){
    # 3x3 confusion matrix --------------------------------
    
    # Accuracy
    ACC = sum(diag(confusion_matrix))/sum(confusion_matrix)
    
    
    return(
      list(
        confusion_matrix = confusion_matrix,
        ACC = ACC
      )
    )
    
  }
  
}
```

---

# Dataset

## 1. iris

### Summary
```{r message=FALSE, warning=FALSE}
# Raw Data
iris %>% DT::datatable()

# Check Variabl Type
iris %>% str()

# Check Missing Value
is.na(iris) %>% DT::datatable()
is.na(iris) %>% sum()

# Check Imbalence Data
table(iris$Species)

# Check Distribution
summary(iris)
```


### Conclusion
- Data : iris (in R)
- Explained Var (Y) : Category (Species)
- Explainatory Var (X) : Continuous 
- Missing Value: 0


Recommended Algorithm: **CART**

---

## 2. Titanic

Titanic in R seems to be an aggregation data, not raw data.

```{r}
data('Titanic')
class(Titanic)
Titanic
```

### Rawdata 

```{r}
Titanic_df = data.frame(Titanic)
repeating_sequence <- rep(1:nrow(Titanic_df), Titanic_df$Freq)
Titanic_raw <- Titanic_df[repeating_sequence,]
Titanic_raw$Freq <- NULL # delete Freq column

nrow(Titanic_df) # Titanic is an aggregation data
nrow(Titanic_raw) 

DT::datatable(Titanic_raw)
```

### Summary
```{r}
# Check Variabl Type
Titanic_raw %>% str()

# Check Missing Value
is.na(Titanic_raw) %>% sum()

# Check Imbalence Data
table(Titanic_raw$Survived)

# Check Distribution
summary(Titanic_raw)
```

### Conclusion
- Data : Titanic (in R)
- Explained Var (Y) : Category (Survived)
- Explainatory Var (X) : Category
- Missing Value: 0

Recommended Algorithm: **ID3**


---

##  3. Breast Cancer

### Raw Data

Source : [UCI : Breast Cancer Data Set](http://archive.ics.uci.edu/ml/datasets/breast+cancer)

```{r message=FALSE, warning=FALSE}

BreastCancer <-  read.csv('http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer/breast-cancer.data', header = FALSE)

DT::datatable(BreastCancer)
```

### Summary

```{r}
BreastCancer.Names <-  readLines('http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer/breast-cancer.names')

BreastCancer.Names
```

### Set Column Name
```{r}
BC_colnames <- c("Class", "age", "menopause", "tumor-size","inv-nodes", 
                 "node-caps", "deg-malig", "breast", "breast-quad", "irradiate")

colnames(BreastCancer) <- BC_colnames

# Check Column Name 
DT::datatable(BreastCancer)

```

### Alter Variable Type
```{r}
# To change numeric value into factor, it should be transferred to chracter first !
BreastCancer$`deg-malig` %<>% as.character() %>% as.factor()

# Check Type Again
str(BreastCancer)
```

### Missing Value

Check "Summary" : 8. Missing Attribute Values: (denoted by "?")

```{r}
# Alter "?" to NA
BreastCancer %<>% 
  as.matrix() %T>% {.[. == "?"] = NA} %>%
  as.data.frame()  

# Missing Value Num
BreastCancer %>% is.na() %>% sum()

```

### Check Imbalance 
```{r}
summary(BreastCancer)
table(BreastCancer$irradiate)
```


### Conclusion
- Data : Breast Cancer (R default)
- Explained Var (Y) : Category(irradiate)
- Explainatory Var (X) : Category
- Total Missing Value : 9
  - breast-quad : 1
  - node-caps : 8
 
Recommended Algorithm: **C4.5, CHAID**


---




---

# Modeling

## ID3

### 0. Setting

<font color="blue" size = 4>Next time, you could just change data and formula in this chunk.</font>

```{r}
data = Titanic_raw
train_proportion = 0.9
set.seed(20200419)
formula = formula(Survived  ~ .)
Y_attribute = 'Survived'

Control.ID3 = rpart::rpart.control(
                      minsplit = 2, # min obs. in node --- too small cause over-fitting
                      minbucket = 1, # min obs. in leaf--- too small cause over-fitting
                      cp = 0, # complex parameter --- too small cause over-fitting
                      xval = 10, # cross validation
                      maxdepth = 30, # The root node counted as depth 0
                      maxcompete = 4, 
                      maxsurrogate = 5, 
                      usesurrogate = 2, 
                      surrogatestyle = 0)
```

### 1. Model

```{r message=FALSE, warning=FALSE}
library(rpart)

dataPartition = Partition(data, train_proportion)

training = dataPartition$training
testing = dataPartition$testing



Tree.ID3 <- rpart(formula = formula, 
                  data = training,
                  method="class",
                  na.action = na.rpart,
                  parms=list(split="information"),
                  control = Control.ID3 
                  )
Tree.ID3

```

---

### 2. Plot

```{r message=FALSE, warning=FALSE}
library(rpart.plot)
rpart.plot(Tree.ID3,
           branch=0, # tree shape
           type = 1, # node shape
           cex=0.8, # sign size
           fallen.leaves=T)

```

---

### 3. Predict

#### Training
```{r message=FALSE, warning=FALSE}
# Apparent Performance
pred.ID3.train <- predict(Tree.ID3, 
                    newdata=training,
                    type="class")

perform.ID3.Apparent <- ModelPerformance(pred.ID3.train, training[,Y_attribute])
## Confusion Matrix
perform.ID3.Apparent$confusion_matrix
## Accuracy
perform.ID3.Apparent$ACC
```


```{r message=FALSE, warning=FALSE}

library(pROC)

pred.ID3.train.prob = predict(Tree.ID3, 
                         newdata=training, 
                         type = "prob")


roc.ID3.train <- roc(response = training[,Y_attribute],
               predictor = pred.ID3.train.prob[,2],
               plot = T,
               print.auc=TRUE,
               auc.polygon=TRUE,
               grid=c(0.1, 0.2),
               grid.col=c("green", "red"),
               max.auc.polygon=TRUE,
               auc.polygon.col="skyblue",
               print.thres=TRUE,
               main='ROC Curve of ID3')


```


---
 
#### Testing
```{r message=FALSE, warning=FALSE}
# True Performance
pred.ID3.test <- predict(Tree.ID3, 
                    newdata=testing,
                    type="class")

perform.ID3.True <- ModelPerformance(pred.ID3.test, testing[,Y_attribute])
## Confusion Matrix
perform.ID3.True$confusion_matrix
## Accuracy
perform.ID3.True$ACC


perform.ID3.True$plot_positive
perform.ID3.True$plot_negative

```


---

### 4. Prune

It seems to be no specific pruning rule in ID3 originally, but we can also apply concept of **predicted error rate** to prune the model.

- To avoid over-fitting, prune tree by proper **cp**.

- To look up a proper cp, look up **lower** cross-validation error(**xerror**). 

- Don't always choose the lowest xerror,**more simplified tree** should also be considered.



<font size=4 color="blue">No over-fitting : This part just shows how prunung works.</font>


```{r}
printcp(Tree.ID3)
```

```{r}
Tree.ID3_prune <- prune(Tree.ID3,cp= 0.01)
Tree.ID3_prune
```

```{r message=FALSE, warning=FALSE}
rpart.plot(Tree.ID3_prune,
           branch=0, # tree shape
           type = 1, # node shape
           cex=0.8, # sign size
           fallen.leaves=T)
```


### 5. Predict (pruned)

#### Training
```{r message=FALSE, warning=FALSE}
# Apparent Performance after pruning
pred.ID3_prune.train <- predict(Tree.ID3_prune,
                    newdata=training,
                    type="class")

perform.ID3_prune.Apparent <- ModelPerformance(pred.ID3_prune.train, training[,Y_attribute])
## Confusion Matrix
perform.ID3_prune.Apparent$confusion_matrix
## Accuracy
perform.ID3_prune.Apparent$ACC
```

```{r message=FALSE, warning=FALSE}

library(pROC)

pred.ID3.train.prob = predict(Tree.ID3, 
                         newdata=training, 
                         type = "prob")


roc.ID3.train <- roc(response = training[,Y_attribute],
               predictor = pred.ID3.train.prob[,2],
               plot = T,
               print.auc=TRUE,
               auc.polygon=TRUE,
               grid=c(0.1, 0.2),
               grid.col=c("green", "red"),
               max.auc.polygon=TRUE,
               auc.polygon.col="skyblue",
               print.thres=TRUE,
               main='ROC Curve of ID3 (prune)')



```

---

#### Testing
```{r message=FALSE, warning=FALSE}
# True Performance after pruning
pred.ID3_prune.test <- predict(Tree.ID3_prune,
                    newdata=testing,
                    type="class")

perform.ID3_prune.True <- ModelPerformance(pred.ID3_prune.test, testing[,Y_attribute])

## Confusion Matrix
perform.ID3_prune.True$confusion_matrix
## Accuracy
perform.ID3_prune.True$ACC

perform.ID3_prune.True$plot_positive
perform.ID3_prune.True$plot_negative



```



---


### 6. Conclusion


| CP Threshold | 0.001 (unpruned) | 0.01 (pruned) |  Trend  |
| ------------ | ---------------- | ------------- | ------- |
|   Training   |      XX.XX%      |     XX.XX%    |    ↑↓   |
|   Testing    |      XX.XX%      |     XX.XX%    |    ↑↓   |



---
---

## C4.5

### Install RWeka
1. If Java hasn't been installed : https://www.java.com/zh_TW/download/win10.jsp
2. Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jdk1.8.0_191\\jre') 
3. install.packages("RWeka")

### 0. Setting

[Weka_control parameters](https://rdrr.io/cran/RWeka/man/Weka_control.html)

```{r message=FALSE, warning=FALSE}
data = BreastCancer
train_proportion = 0.9
set.seed(20200418)
formula = formula(irradiate ~ .)
Y_attribute = 'irradiate'

Control.C45.Unprune = RWeka::Weka_control(
                       U = TRUE,  # Use unpruned tree
                       R= FALSE, # Use reduced error pruning (REP)
                       M=2, # Minimum number of instances per leaf
                       B=FALSE # Use binary splits only
                       )

Control.C45.Prune = RWeka::Weka_control(
                       U = FALSE,  
                       R= TRUE, 
                       M=2, 
                       B=FALSE 
                       )

```


### 1. Model

```{r message=FALSE, warning=FALSE}
library(RWeka)
library(pROC) 

dataPartition = Partition(data, train_proportion)

training = dataPartition$training
testing = dataPartition$testing


Tree.C45 <- J48(formula = formula,
                data = training,
                control = Control.C45.Prune)


```

### 2. Plot

```{r message=FALSE, warning=FALSE}
require(partykit)
plot(Tree.C45)
```

### 3. Predict

#### Training
```{r message=FALSE, warning=FALSE}
pred.C45.train = predict(Tree.C45, 
                         newdata=training, 
                         type = "class")

perform.C45.Apparent <- ModelPerformance(pred.C45.train, training[,Y_attribute])
perform.C45.Apparent$confusion_matrix
perform.C45.Apparent$ACC
```


```{r message=FALSE, warning=FALSE}

library(pROC)

pred.C45.train.prob = predict(Tree.C45, 
                         newdata=training, 
                         type = "prob")


roc.C45.train <- roc(response = training[,Y_attribute],
               predictor = pred.C45.train.prob[,2],
               plot = T,
               print.auc=TRUE,
               auc.polygon=TRUE,
               grid=c(0.1, 0.2),
               grid.col=c("green", "red"),
               max.auc.polygon=TRUE,
               auc.polygon.col="skyblue",
               print.thres=TRUE,
               main='ROC Curve of C4.5')



roc.C45.train
```



---

#### Testing
```{r message=TRUE, warning=FALSE}
# True Performance
pred.C45.test <- predict(Tree.C45, 
                    newdata=testing,
                    type="class")

perform.C45.True <- ModelPerformance(pred.C45.test, testing[,Y_attribute])
perform.C45.True$confusion_matrix
perform.C45.True$ACC


perform.C45.True$plot_positive
perform.C45.True$plot_negative
```



### 4. Conclusion


---

## CART

### 0. Setting
```{r message=FALSE, warning=FALSE}
set.seed(20200418)
data = iris
train_proportion = 0.9
formula = formula(Species  ~ .)
Y_attribute = 'Species'

Control.CART = rpart::rpart.control(
                      minsplit = 8, # min obs. in node --- too small cause over-fitting
                      minbucket = 1, # min obs. in leaf--- too small cause over-fitting
                      cp = 0, # complex parameter --- too small cause over-fitting
                      xval = 10, # cross validation
                      maxdepth = 30, # The root node counted as depth 0
                      maxcompete = 4, 
                      maxsurrogate = 5, 
                      usesurrogate = 2, 
                      surrogatestyle = 0)
```


### 1. Model

```{r message=FALSE, warning=FALSE}
library(rpart)

dataPartition = Partition(data, train_proportion)

training = dataPartition$training
testing = dataPartition$testing



Tree.CART <- rpart(formula = formula, 
                  data = training,
                  method="class",
                  na.action = na.rpart,
                  parms=list(split="gini"),
                  control = Control.CART 
                  )
Tree.CART

```

---

### 2. Plot

```{r message=FALSE, warning=FALSE}
library(rpart.plot)
rpart.plot(Tree.CART,
           branch=0, # tree shape
           type = 1, # node shape
           cex=0.8, # sign size
           fallen.leaves=T)

```

---

### 3. Predict

#### Training
```{r message=FALSE, warning=FALSE}
# Apparent Performance
pred.CART.train <- predict(Tree.CART, 
                    newdata=training,
                    type="class")

perform.CART.Apparent <- ModelPerformance(pred.CART.train, training[,Y_attribute])

## Confusion Matrix
perform.CART.Apparent$confusion_matrix

## Accuracy
perform.CART.Apparent$ACC
```

```{r message=FALSE, warning=FALSE}

library(pROC)

pred.CART.train.prob = predict(Tree.CART, 
                         newdata=training, 
                         type = "prob")


roc.CART.train <- multiclass.roc(response = training[,Y_attribute],
               predictor = pred.CART.train.prob,
               plot = T,
               print.auc=TRUE,
               auc.polygon=TRUE,
               grid=c(0.1, 0.2),
               grid.col=c("green", "red"),
               max.auc.polygon=TRUE,
               auc.polygon.col="skyblue",
               print.thres=TRUE,
               main='ROC Curve of CART')

roc.CART.train$rocs

```


---
 
#### Testing
```{r message=FALSE, warning=FALSE}
# True Performance
pred.CART.test <- predict(Tree.CART, 
                    newdata=testing,
                    type="class")

perform.CART.True <- ModelPerformance(pred.CART.test, testing[,Y_attribute])

## Confusion Matrix
perform.CART.True$confusion_matrix

## Accuracy
perform.CART.True$ACC



```



---


### 4. Prune


- To look up a proper cp, look up **lower** cross-validation error(**xerror**). 

- Don't always choose the lowest xerror,**more simplified tree** should also be considered.

- After over-fitting,try pruning the tree by **larger** cp.



```{r}
printcp(Tree.CART)
```

```{r}
Tree.CART_prune <- prune(Tree.CART,cp= 0.02) 
Tree.CART_prune
```

```{r message=FALSE, warning=FALSE}
rpart.plot(Tree.CART_prune,
           branch=0, # tree shape
           type = 1, # node shape
           cex=0.8, # sign size
           fallen.leaves=T)
```


### 5. Predict (pruned)

#### Training
```{r message=FALSE, warning=FALSE}
# Apparent Performance after pruning
pred.CART_prune.train <- predict(Tree.CART_prune,
                    newdata=training,
                    type="class")

perform.CART_prune.Apparent <- ModelPerformance(pred.CART_prune.train, training[,Y_attribute])
## Confusion Matrix
perform.CART_prune.Apparent$confusion_matrix
## Accuracy
perform.CART_prune.Apparent$ACC
```


```{r message=FALSE, warning=FALSE}

library(pROC)

pred.CART_prune.train.prob = predict(Tree.CART_prune, 
                         newdata=training, 
                         type = "prob")

## There are 3 classes in Species of iris.
roc.CART_prune.train <- multiclass.roc(response = training[,Y_attribute],
               predictor = pred.CART_prune.train.prob,
               plot = T,
               print.auc=TRUE,
               auc.polygon=TRUE,
               grid=c(0.1, 0.2),
               grid.col=c("green", "red"),
               max.auc.polygon=TRUE,
               auc.polygon.col="skyblue",
               print.thres=TRUE,
               main='ROC Curve of CART')

roc.CART_prune.train$rocs
```


---

#### Testing
```{r message=FALSE, warning=FALSE}
# True Performance after pruning
pred.CART_prune.test <- predict(Tree.CART_prune,
                    newdata=testing,
                    type="class")

perform.CART_prune.True <- ModelPerformance(pred.CART_prune.test, testing[,Y_attribute])

## Confusion Matrix
perform.CART_prune.True$confusion_matrix
## Accuracy
perform.CART_prune.True$ACC


```


---


### 6. Conclusion

| CP Threshold |   0 (unpruned)   | 0.02 (pruned) |    Trend     |
| ------------ | ---------------- | ------------- | ------------ |
|   Training   |      98.52%      |     97.77%    |  ↓ (-0.75%)  |
|   Testing    |      86.67%      |     93.33%    |  ↑ (+6.66%)  |




---

## C5.0

C5.0Control Parameters:https://cran.r-project.org/web/packages/C50/C50.pdf 

### 0. Setting
```{r message=FALSE, warning=FALSE}
data = BreastCancer
train_proportion = 0.9
set.seed(20200418)
formula = formula(irradiate  ~ .)
Y_attribute = 'irradiate'


Control.C50 =C50::C5.0Control(
                 subset = FALSE, 
                 bands = 0, # when betewwn 2 and 1000, rules = TRUE
                 winnow = FALSE,
                 noGlobalPruning = TRUE, # The final global pruning step 
                 CF = 0.25, 
                 minCases = 2,
                 fuzzyThreshold = FALSE,
                 sample = 0,
                 seed = sample.int(4096, size = 1) -1L,
                 earlyStopping = TRUE
                 )



```

### 1. Model
```{r}
library(C50)
dataPartition = Partition(data, train_proportion)

training = dataPartition$training
testing = dataPartition$testing


Tree.C50 <- C5.0(x = dplyr::select(training, -Y_attribute), 
                 y = training[,Y_attribute],
                 control =Control.C50,
                 trials = 10, # boosting iteration
                 rules = FALSE
                 )

summary(Tree.C50) 
```

### 2. Plot
```{r}
plot(Tree.C50)
```


### 3. Predict

#### Training
```{r message=FALSE, warning=FALSE}
# Apparent Performance
pred.C50.train <- predict(Tree.C50, 
                    newdata=training,
                    type="class")

perform.C50.Apparent <- ModelPerformance(pred.C50.train, training[,Y_attribute])

## Confusion Matrix
perform.C50.Apparent$confusion_matrix

## Accuracy
perform.C50.Apparent$ACC
```

```{r message=FALSE, warning=FALSE}

library(pROC)

pred.C50.train.prob = predict(Tree.C50, 
                         newdata=training, 
                         type = "prob")


roc.C50.train <- roc(response = training[,Y_attribute],
               predictor = pred.C50.train.prob[,2],
               plot = T,
               print.auc=TRUE,
               auc.polygon=TRUE,
               grid=c(0.1, 0.2),
               grid.col=c("green", "red"),
               max.auc.polygon=TRUE,
               auc.polygon.col="skyblue",
               print.thres=TRUE,
               main='ROC Curve of CART')

roc.C50.train

```

---

#### Testing
```{r message=FALSE, warning=FALSE}
# True Performance
pred.C50.test <- predict(Tree.C50, 
                    newdata=testing,
                    type="class")

perform.C50.True <- ModelPerformance(pred.C50.test, testing[,Y_attribute])

## Confusion Matrix
perform.C50.True$confusion_matrix

## Accuracy
perform.C50.True$ACC

perform.C45.True$plot_positive
perform.C45.True$plot_negative

```


### 4. Conclusion

---

## CHAID

How to install CHAID package:
+ Check CHAID version: http://download.r-forge.r-project.org/src/contrib/
+ install.packages("http://download.r-forge.r-project.org/src/contrib/CHAID_0.1-2.tar.gz", repos = NULL)


### 0. Setting
```{r message=FALSE, warning=FALSE}
data = BreastCancer
train_proportion = 0.9
set.seed(20200418)
formula = formula(irradiate  ~ .)
Y_attribute = 'irradiate'


Control.CHAID = CHAID::chaid_control(
                      alpha2 = 0.05, # for merging
                      alpha3 = 0.01,  # for spliting
                      alpha4 = 0.05, # for feature selection
                      minsplit = 10, 
                      minbucket = 3, 
                      maxheight = 30, 
                      minprob = 0.01,  # min proportion in leaf node (to total sample)
                      stump = FALSE # only split root node
                     )



```



### 1. Model
```{r message=FALSE, warning=FALSE}
library(CHAID)

dataPartition = Partition(data, train_proportion)

training = dataPartition$training
testing = dataPartition$testing

Tree.CHAID <- chaid(formula = formula, 
                    data = training, 
                    control = Control.CHAID)

Tree.CHAID

```

### 2. Plot

```{r}
plot(Tree.CHAID)
```


### 3. Predict

#### Training
```{r message=FALSE, warning=FALSE}
# Apparent Performance
pred.CHAID.train <- predict(Tree.CHAID, 
                    newdata=training,
                    type="response")

perform.CHAID.Apparent <- ModelPerformance(pred.CHAID.train, training[,Y_attribute])

## Confusion Matrix
perform.CHAID.Apparent$confusion_matrix

## Accuracy
perform.CHAID.Apparent$ACC
```

```{r message=FALSE, warning=FALSE}

library(pROC)

pred.CHAID.train.prob = predict(Tree.CHAID, 
                         newdata=training, 
                         type = "prob")


roc.CHAID.train <- roc(response = training[,Y_attribute],
               predictor = pred.CHAID.train.prob[,2],
               plot = T,
               print.auc=TRUE,
               auc.polygon=TRUE,
               grid=c(0.1, 0.2),
               grid.col=c("green", "red"),
               max.auc.polygon=TRUE,
               auc.polygon.col="skyblue",
               print.thres=TRUE,
               main='ROC Curve of CART')

roc.CHAID.train

```


---
 
#### Testing
```{r message=FALSE, warning=FALSE}
# True Performance
pred.CHAID.test <- predict(Tree.CHAID, 
                    newdata=testing,
                    type="response")

perform.CHAID.True <- ModelPerformance(pred.CHAID.test, testing[,Y_attribute])

## Confusion Matrix
perform.CHAID.True$confusion_matrix
## Accuracy
perform.CART.True$ACC

perform.CHAID.True$plot_positive
perform.CHAID.True$plot_negative
```


### 4. Conclusion






## ------------------------------



### 0. Setting

### 1. Model

### 2. Plot

### 3. Predict

#### Training

#### Testing

### 4. Prune

### 5. Predict (prune)

### 6. Conclusion
