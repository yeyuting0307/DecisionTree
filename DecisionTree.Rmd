---
title: "DecisionTree"
author: "Mike"
date: "2020/4/6"
output: 
  html_document: 
    theme: united
    toc: yes
    toc_float: yes
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Reference

1. [**(推薦)**决策树（decision tree）(一)——构造决策树方法](https://blog.csdn.net/u012328159/article/details/70184415)

2. [**(推薦)**决策树（decision tree）(二)——剪枝](https://blog.csdn.net/u012328159/article/details/79285214)

3. [**(推薦)**决策树（decision tree）(三)——连续值处理](https://blog.csdn.net/u012328159/article/details/79396893)

4. [**(推薦)**决策树（decision tree）（四）——缺失值处理](https://blog.csdn.net/u012328159/article/details/79413610)

5.  [Decision Tree 決策樹 | CART, Conditional Inference Tree, RandomForest](https://www.jamleecute.com/decision-tree-cart-%E6%B1%BA%E7%AD%96%E6%A8%B9/)

6. [機器學習：決策樹總結｜ID3 C4.5/C5.0 CHAID CART與QUEST](https://kknews.cc/zh-tw/tech/m36obez.html)

7. [決策樹(Decision Tree)以及隨機森林(Random Forest)介紹](https://medium.com/jameslearningnote/%E8%B3%87%E6%96%99%E5%88%86%E6%9E%90-%E6%A9%9F%E5%99%A8%E5%AD%B8%E7%BF%92-%E7%AC%AC3-5%E8%AC%9B-%E6%B1%BA%E7%AD%96%E6%A8%B9-decision-tree-%E4%BB%A5%E5%8F%8A%E9%9A%A8%E6%A9%9F%E6%A3%AE%E6%9E%97-random-forest-%E4%BB%8B%E7%B4%B9-7079b0ddfbda)

8. [HOW DECISION TREE ALGORITHM WORKS](http://dataaspirant.com/2017/01/30/how-decision-tree-algorithm-works/)

9. [C4.5 Pessimistic Error Pruning，PEP](http://gitlinux.net/2019-06-04-C45/)

10. [决策树建树原理之C4.5和C5.0以及CART建树原理（二](https://blog.csdn.net/weixin_44835596/article/details/89852921)

11. [IBM SPSS Modeler算法系列--决策树CHAID算法](https://www.cda.cn/view/20421.html)

12. [IBM SPSS Modeler算法系列-----决策树C5.0算法](https://blog.csdn.net/chenjunji123456/article/details/52189312?depth_1-utm_source=distribute.pc_relevant.none-task-blog-OPENSEARCH-12&utm_source=distribute.pc_relevant.none-task-blog-OPENSEARCH-12)

13. [CART 分類與回歸樹](https://www.itread01.com/content/1509178954.html)


14. [Regression Tree | 迴歸樹, Bagging, Bootstrap Aggregation | R語言](https://www.jamleecute.com/regression-tree-%E8%BF%B4%E6%AD%B8%E6%A8%B9-bagging-bootstrap-aggrgation-r%E8%AA%9E%E8%A8%80/)

15. [rpart 决策树中的 Cp（complexity parameter）参数](https://blog.csdn.net/a8131357leo/article/details/89529677)

16. [R语言：决策树ID3/C4.5/CART/C5.0算法的实现](https://blog.csdn.net/weixin_43216017/article/details/87739323)


17. [决策树ID3原理及R语言python代码实现（西瓜书）](https://www.bbsmax.com/A/ke5jv4wy5r/)

18. [NTU CINC : R軟體資料探勘實務(上)--分類模型](http://www.cc.ntu.edu.tw/chinese/epaper/0034/20150920_3410.html)


19. [Meaning of Surrogate Split](https://stats.stackexchange.com/questions/171574/meaning-of-surrogate-split)

---

# Algorithm

| Algorithm | Explainatory Var. Type |       Feature Selection       | Pruning Rule | Missing Value |
| --------- | ---------------------- | ----------------------------- | ------------ | ------------- |
|    ID3    |       Discrete         |    Entropy, Information Gain  |  No Pruning  |  Not Allowed  |
|   C4.5    | Discrete & Continueous |    Entropy, IV, Gain Ratio    |     PEP      |    Allowed    |
|   CHAID   | Discrete & Continueous |    Chisquare Test p value     |      ?       |       ?       |
|   CART    | Discrete & Continueous |       Gini Index, MSE         |  No Pruning  |       ?       |
|   QUEST   | Discrete & Continueous |  Chisquare / F Test p value   |  No Pruning  |       ?       |
|   C5.0    | Discrete & Continueous | Gain Ratio, PRISM?, Boosting? |     PEP      |       ?       |
|  PUBLIC   |          ?             |               ?               |      ?       |       ?       |
|   SLIQ    |          ?             |               ?               |      ?       |       ?       |
|  SPRINT   |          ?             |               ?               |      ?       |       ?       |



# Packages & Functions

```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=FALSE}
library(magrittr) # %<>%
library(dplyr) # data manipulation
library(rpart) # ID3, CART
# library(RWeka) # C4.5(J48)
# library(C50) # C5.0
```

## Partition Data 

Ramdomly partition data into Training and Testing
```{r}
Partition = function(data, train_proportion = 0.9){
  train_index = sample(x = nrow(data), 
                       size = train_proportion*nrow(data),
                       replace = FALSE )
  train_data = data[train_index,]
  test_data = data[-train_index,]
  return(
    list(
      training = train_data,
      testing = test_data
      )
    )
}
  

```

## Model Performance

```{r message=FALSE, warning=FALSE}

ModelPerformance <- function(predict_condition, true_condition){
  
  require(dplyr)
  require(tidyr)
  require(ggplot2)
    
  # Confusion Matrix, Contingency Table
  confusion_matrix <- table(predict_condition, true_condition)
  
  # Hit, True Positive
  TP <-  confusion_matrix[1,1]
  # Correct Rejection, True Negative
  TN <- confusion_matrix[2,2]
  # False Alarm, Type I Error, False Positive
  FP <-  confusion_matrix[1,2]
  # Miss, Type II Error, False Negative
  FN <-  confusion_matrix[2,1]
  
  
  
  # Sensitivity, Recall, True Positive Rate
  TPR <- TP / (TP + FN)
  
  # Specificity, Selectivity, True Negative Rate
  TNR <- TN / (TN + FP)
  
  # Precision, Positive Predictive Value
  PPV <- TP / (TP + FP)
  
  # Negative Prediction Value
  NPV <- TN / (TN + FN)
  
  # Miss Rate, False Negative Rate
  FNR <- FN /(FN + TP)
  
  # Fall Out, False Positive Rate
  FPR <-  FP / (FP + TN)
  
  # False Discovery Rate
  FDR <- FP / (FP + TP)
  
  # False Omission Rate
  FOR <- FN / (FN + TN)
  
  
  
  # Threat Score, Critical Success Index
  TS <- TP / (TP + FN + FP)
  
  # Accuracy
  ACC <- (TP + TN) / sum(confusion_matrix)
  
  # Balanced Accuracy, Harmonic Mean of Precision and Sensitivity
  BA <- (TPR + TNR) / 2
  
  # F1 Score
  F1 <- 2*(PPV*TPR)/(PPV+TPR)
  
  # Matthews correlation coefficient
  MCC <- (TP*TN -FP*FN) / (sqrt(TP+FP)*sqrt(TP+FN)*sqrt(TN+FP)*sqrt(TN+FN))
  
  # Bookmaker Informedness
  BM <- TPR + TNR - 1
  
  # Markedness, delta-P
  MK <- PPV + NPV - 1
  
  df = data.frame(
        TPR = TPR, 
        TNR = TNR, 
        PPV = PPV, 
        NPV = NPV, 
        FNR = FNR, 
        FPR = FPR, 
        FDR = FDR, 
        FOR = FOR, 
        TS = TS, 
        ACC = ACC, 
        BA = BA, 
        F1 = F1, 
        MCC = MCC, 
        BM = BM, 
        MK = MK
      )
  
  
  positive_measure = df[c('TPR', 'TNR', 'PPV', 'NPV', 'F1',
                          'ACC', 'TS', 'BA', "BM", 'MK')]%>% gather

  plot_positive <-  ggplot(positive_measure, 
       aes(x=reorder(key,-value),y=value,fill=key))+
        geom_bar(stat="identity")+
        # coord_polar(theta="x",direction=1)+
        labs(x="Measure",y="Performance")+
        theme(legend.position="bottom",legend.box="horizontal")+
        ggtitle(label = 'Positive Model Performance',
                subtitle = 'The higher, the better.')+
        geom_text(aes(x = reorder(key,-value),
                            y=value, 
                            label = round(value, 2)),
                  vjust = 1.2)
  
  
  negative_measure = df[c("FNR", "FPR", "FDR", "FOR")]%>% gather

  plot_negative <- ggplot(negative_measure, 
       aes(x=reorder(key,-value),y=value,fill=key))+
        geom_bar(stat="identity")+
        # coord_polar(theta="x",direction=1)+
        labs(x="Measure",y="Performance")+
        theme(legend.position="bottom",legend.box="horizontal")+
        ggtitle(label = 'Negative Model Performance',
                subtitle = 'The lower, the better.')+
        geom_text(aes(x = reorder(key,-value),
                            y=value, 
                            label = round(value, 2)),
                  vjust = 1.2)


  return(
    list(
      confusion_matrix = confusion_matrix,
      TP = TP, 
      TN = TN, 
      FP = FP, 
      FN = FN, 
      
      TPR = TPR, 
      TNR = TNR, 
      PPV = PPV, 
      NPV = NPV, 
      FNR = FNR, 
      FPR = FPR, 
      FDR = FDR, 
      FOR = FOR, 
      TS = TS, 
      ACC = ACC, 
      BA = BA, 
      F1 = F1, 
      MCC = MCC, 
      BM = BM, 
      MK = MK,
  
      df = df,
      plot_positive = plot_positive,
      plot_negative = plot_negative
    )
  )
  
}
```

---

# Dataset

## 1. iris

### Summary
```{r message=FALSE, warning=FALSE}
# Raw Data
iris %>% DT::datatable()

# Check Variabl Type
iris %>% str()

# Check Missing Value
is.na(iris) %>% DT::datatable()
is.na(iris) %>% sum()

# Check Imbalence Data
table(iris$Species)

# Check Distribution
summary(iris)
```


### Conclusion
- Data : iris (in R)
- Explained Var (Y) : Category (Species)
- Explainatory Var (X) : Continuous 
- Missing Value: 0


Recommended Algorithm: **CART**

---

## 2. Titanic

Titanic in R seems to be an aggregation data, not raw data.

```{r}
data('Titanic')
class(Titanic)
Titanic
```

### Rawdata 

```{r}
Titanic_df = data.frame(Titanic)
repeating_sequence <- rep(1:nrow(Titanic_df), Titanic_df$Freq)
Titanic_raw <- Titanic_df[repeating_sequence,]
Titanic_raw$Freq <- NULL # delete Freq column

nrow(Titanic_df) # Titanic is an aggregation data
nrow(Titanic_raw) 

DT::datatable(Titanic_raw)
```

### Summary
```{r}
# Check Variabl Type
Titanic_raw %>% str()

# Check Missing Value
is.na(Titanic_raw) %>% sum()

# Check Imbalence Data
table(Titanic_raw$Survived)

# Check Distribution
summary(Titanic_raw)
```

### Conclusion
- Data : Titanic (in R)
- Explained Var (Y) : Category (Survived)
- Explainatory Var (X) : Category
- Missing Value: 0

Recommended Algorithm: **ID3**


---

##  3. Breast Cancer

### Raw Data

Source : [UCI : Breast Cancer Data Set](http://archive.ics.uci.edu/ml/datasets/breast+cancer)

```{r message=FALSE, warning=FALSE}

BreastCancer <-  read.csv('http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer/breast-cancer.data', header = FALSE)

DT::datatable(BreastCancer)
```

### Summary

```{r}
BreastCancer.Names <-  readLines('http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer/breast-cancer.names')

BreastCancer.Names
```

### Set Column Name
```{r}
BC_colnames <- c("Class", "age", "menopause", "tumor-size","inv-nodes", 
                 "node-caps", "deg-malig", "breast", "breast-quad", "irradiate")

colnames(BreastCancer) <- BC_colnames

# Check Column Name 
DT::datatable(BreastCancer)

```

### Alter Variable Type
```{r}
# To change numeric value into factor, it should be transferred to chracter first !
BreastCancer$`deg-malig` %<>% as.character() %>% as.factor()

# Check Type Again
str(BreastCancer)
```

### Missing Value

Check "Summary" : 8. Missing Attribute Values: (denoted by "?")

```{r}
# Alter "?" to NA
BreastCancer %<>% 
  as.matrix() %T>% {.[. == "?"] = NA} %>%
  as.data.frame()  

# Missing Value Num
BreastCancer %>% is.na() %>% sum()

```

### Check Imbalance 
```{r}
summary(BreastCancer)
table(BreastCancer$irradiate)
```


### Conclusion
- Data : iris(R default)
- Explained Var (Y) : Category(irradiate)
- Explainatory Var (X) : Category
- Total Missing Value : 9
  - breast-quad : 1
  - node-caps : 8
 
Recommended Algorithm: **C4.5**


---




---

# Modeling

## ID3

### 0. Setting

<font color="blue" size = 4>Next time, you could just change data and formula in this chunk.</font>

```{r}
set.seed(20200419)
data = Titanic_raw
train_proportion = 0.9
formula = formula(Survived  ~ .)
Y_attribute = 'Survived'

```

### 1. Model

```{r message=FALSE, warning=FALSE}
library(rpart)

dataPartition = Partition(data, train_proportion)

training = dataPartition$training
testing = dataPartition$testing


Control.ID3 = rpart.control(minsplit = 10, # min obs. in node
                            minbucket = 3, # min obs. in leaf, defaut is round(minsplit/3)
                            cp = 0.001, # Lower cp first. Later prune tree by larger one.
                            maxcompete = 4, # ???
                            maxsurrogate = 5, # ???
                            usesurrogate = 2, # ???
                            xval = 10, # cross validation
                            surrogatestyle = 0, # ???
                            maxdepth = 30 #The root node counted as depth 0
                            )


Tree.ID3 <- rpart(formula = formula, 
                  data = training,
                  method="class",
                  na.action = na.rpart,
                  parms=list(split="information"),
                  control = Control.ID3 
                  )
Tree.ID3

```

---

### 2. Plot

```{r message=FALSE, warning=FALSE}
library(rpart.plot)
rpart.plot(Tree.ID3,
           branch=0, # tree shape
           type = 1, # node shape
           cex=0.8, # sign size
           fallen.leaves=T)

```

---

### 3. Predict (unpruned)

#### Training
```{r message=FALSE, warning=FALSE}
# Apparent Performance
pred.ID3.train <- predict(Tree.ID3, 
                    newdata=training,
                    type="class")

perform.ID3.Apparent <- ModelPerformance(pred.ID3.train, training[,Y_attribute])
perform.ID3.Apparent$confusion_matrix
perform.ID3.Apparent$ACC
```

<font size=4 color="blue">Apparent accuracy is not very high, it may not be over-fitting.</font>

---
 
#### Testing
```{r message=FALSE, warning=FALSE}
# True Performance
pred.ID3.test <- predict(Tree.ID3, 
                    newdata=testing,
                    type="class")

perform.ID3.True <- ModelPerformance(pred.ID3.test, testing[,Y_attribute])
perform.ID3.True$confusion_matrix
perform.ID3.True$ACC


perform.ID3.True$plot_positive
perform.ID3.True$plot_negative

```

---

### 4. Prune

To avoid over-fitting, prune tree by proper cp.

To look up a proper cp, one way is to look up **lower** cross-validation error(**xerror**). 

<font size=4 color="blue">Not over-fitting : This part just shows how prunung works.</font>


```{r}
printcp(Tree.ID3)
```

```{r}
Tree.ID3_prune <- prune(Tree.ID3,cp= 0.01)
Tree.ID3_prune
```

```{r message=FALSE, warning=FALSE}
rpart.plot(Tree.ID3_prune,
           branch=0, # tree shape
           type = 1, # node shape
           cex=0.8, # sign size
           fallen.leaves=T)
```


---

### 5. Predict (pruned)

#### Training
```{r message=FALSE, warning=FALSE}
# Apparent Performance after pruning
pred.ID3_prune.train <- predict(Tree.ID3_prune, 
                    newdata=training,
                    type="class")

perform.ID3_prune.Apparent <- ModelPerformance(pred.ID3_prune.train, training[,Y_attribute])

perform.ID3_prune.Apparent$confusion_matrix
perform.ID3_prune.Apparent$ACC
```

---

#### Testing
```{r message=FALSE, warning=FALSE}
# True Performance after pruning
pred.ID3_prune.test <- predict(Tree.ID3_prune, 
                    newdata=testing,
                    type="class")

perform.ID3_prune.True <- ModelPerformance(pred.ID3_prune.test, testing[,Y_attribute])

perform.ID3_prune.True$confusion_matrix
perform.ID3_prune.True$ACC

perform.ID3_prune.True$plot_positive
perform.ID3_prune.True$plot_negative



```

---

### 6. Conclusion


| CP Threshold | 0.001 (unpruned) | 0.01 (pruned) |  Trend |
| ------------ | ---------------- | ------------- | ------ |
|   Training   |      XX.XX%      |     XX.XX%    |    ↓   |
|   Testing    |      XX.XX%      |     XX.XX%    |    ↑   |



---
---

## C4.5

### Install RWeka
1. If Java hasn't been installed : https://www.java.com/zh_TW/download/win10.jsp
2. Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jdk1.8.0_191\\jre') 
3. install.packages("RWeka")

```{r}
library(RWeka)
library(pROC) 
```

## C5.0


## CHAID










